{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `precip-dot` data test `03`:\n",
    "## Ability to independently re-produce pipeline\n",
    "\n",
    "Verify that data produced at each step throughout the pipeline match what is expected with independent computation. \n",
    "\n",
    "### inputs\n",
    "\n",
    "The path to the directory containing the output directories for all steps of the pipeline needs to be saved in the `PIPE_DIR`. A set of sample locations will be used, represented as latitude and longitude values. Those values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test coordinates that will be used: [(-147.7164, 64.8378), (-149.9003, 61.2181)]\n"
     ]
    }
   ],
   "source": [
    "wgs84_coords = [\n",
    "    (-147.7164, 64.8378), # Fairbanks\n",
    "    (-149.9003, 61.2181) # Anchorage\n",
    "]\n",
    "print(\"Test coordinates that will be used:\", wgs84_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests\n",
    "\n",
    "Each of the tests is enclosed in a function in the next cell, as are some helper functions. Generally, the flow of  tests go as: compute manually for sample locations, compare to pipeline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(func, args):\n",
    "    tic = time.perf_counter()\n",
    "    out = func(*args)\n",
    "    elapsed = time.perf_counter() - tic\n",
    "    d,u = 1,\"s\"\n",
    "    if elapsed > 60:\n",
    "        d,u = 60,\"m\"\n",
    "    print(f\"Elapsed time: {round(elapsed / d, 1)} {u}\\n\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def transform_wgs84(coords_lst, wrf=True):\n",
    "    \"\"\"\n",
    "    Transform list of lon, lat coordinates to desired projection\n",
    "    \"\"\"\n",
    "    if wrf:\n",
    "        wrf_proj = \"+units=m +proj=stere +lat_ts=64.0 +lon_0=-152.0 +lat_0=90.0 +x_0=0 +y_0=0 +a=6370000 +b=6370000\"\n",
    "        transformer = Transformer.from_proj(\"EPSG:4326\", wrf_proj, always_xy=True)\n",
    "    else:\n",
    "        transformer = Transformer.from_crs(4326, 3338, always_xy=True)\n",
    "    \n",
    "    return [transformer.transform(*coords) for coords in coords_lst]\n",
    "\n",
    "\n",
    "def read_wrf(args):\n",
    "    \"\"\"\n",
    "    Read and return xarray.DataArrays from a WRF grid-based netCDF file\n",
    "    Set up for using Pool\n",
    "    \"\"\"\n",
    "    fp, sel_xy = args[0], args[1]\n",
    "    with xr.open_dataset(fp) as ds:\n",
    "        varnames = [key for key in ds.data_vars.keys()]\n",
    "        if len(varnames) == 1:\n",
    "            das = [\n",
    "                ds.sel(xc=xy[0], yc=xy[1], method=\"nearest\")[varnames[0]]\n",
    "                for xy in sel_xy\n",
    "            ]\n",
    "        else:\n",
    "            # return dataset if intervals step or later\n",
    "            das = [\n",
    "                ds.sel(xc=xy[0], yc=xy[1], method=\"nearest\")   \n",
    "                for xy in sel_xy\n",
    "            ]\n",
    "            \n",
    "    return das\n",
    "\n",
    "\n",
    "def read_a14(args):\n",
    "    \"\"\"\n",
    "    Read and return single values by location from Atlas 14 \n",
    "    GeoTIFFs\n",
    "    \"\"\"\n",
    "    fp, windows = args[0], args[1]\n",
    "    with rio.open(fp) as src:\n",
    "        return [src.read(1, window=window)[0][0]/1000 for window in windows]\n",
    "\n",
    "    \n",
    "def run_durations_test(wrf_dir, dur_dir, wrf_xy):\n",
    "    \"\"\"\n",
    "    Verify that durations series are reproducible for sample indices\n",
    "    Return results and durations data for use in next test\n",
    "    \"\"\"    \n",
    "    # test for all 4 data groups\n",
    "    results = {}\n",
    "    for group in DATA_GROUPS:\n",
    "        # read and compute sums manually from WRF data\n",
    "        print(f\"Working on {group}...\", end=\"\")\n",
    "        # read target comparison year and previous year in case of overlap of binning periods\n",
    "        year = DATA_GROUPS[group][\"dur\"]\n",
    "        wrf_fp = sorted(glob.glob(os.path.join(wrf_dir, f\"*{group}*{year}.nc\")))[0]\n",
    "        wrf_args = (wrf_fp, wrf_xy)\n",
    "        wrf_das = read_wrf(wrf_args)\n",
    "        man_arrs = np.array([\n",
    "            np.array([\n",
    "                da.resample(time=duration).sum().values[:5] \n",
    "                for duration in DURATIONS_PANDAS\n",
    "            ])\n",
    "            for da in wrf_das\n",
    "        ])\n",
    "                \n",
    "        # read durations sums from pipeline step\n",
    "        dur_fps = []\n",
    "        # construct filepaths\n",
    "        for duration in DURATIONS:\n",
    "            if duration in [\"60m\", \"2h\", \"3h\", \"6h\"]:\n",
    "                year = year\n",
    "            else:\n",
    "                year = \"\"\n",
    "            fp = sorted(glob.glob(os.path.join(dur_dir, f\"*_{duration}*{group}*{year}.nc\")))[0]\n",
    "            dur_fps.append(fp)        \n",
    "        dur_args = [(fp, wrf_xy) for fp in dur_fps]\n",
    "        p = Pool(16)\n",
    "        dur_das = p.map(read_wrf, dur_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        # make arrays that are comparable to manually computed results\n",
    "        dur_arrs = np.array([\n",
    "            np.array([da_lst[i].values[:5] for da_lst in dur_das])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "        \n",
    "        result = np.all(np.isclose(dur_arrs, man_arrs))\n",
    "        \n",
    "        results[group] = {\"result\": result, \"dur_das\": dur_das}\n",
    "        if result:\n",
    "            print(f\"Result: PASS\")\n",
    "        else:\n",
    "            print(f\"Result: FAIL\")\n",
    "            \n",
    "    return results\n",
    "        \n",
    "    \n",
    "    \n",
    "def run_ams_test(dur_results, dur_dir, ams_dir, wrf_xy):\n",
    "    \"\"\"\n",
    "    Verify that annual maximum series are reproducible for sample locations\n",
    "    Takes output dict from durations test\n",
    "    \"\"\"\n",
    "    # test for all 4 data groups\n",
    "    results = {}\n",
    "    for group in DATA_GROUPS:\n",
    "        # compute AMS manually from durations test output\n",
    "        print(f\"Working on {group}...\", end=\"\")\n",
    "        dur_das = dur_results[group][\"dur_das\"]\n",
    "        # iterate through short durations to replace loaded \n",
    "        #  durations data with first year of ams series depending on group\n",
    "        year = DATA_GROUPS[group][\"ams\"]\n",
    "        short_dur_fps = [\n",
    "            sorted(glob.glob(os.path.join(dur_dir, f\"*_{duration}*{group}*{year}.nc\")))[0]\n",
    "            for duration in [\"60m\", \"2h\", \"3h\", \"6h\"]\n",
    "        ]\n",
    "        short_dur_args = [(fp, wrf_xy) for fp in short_dur_fps]\n",
    "        p = Pool(4)\n",
    "        short_dur_das = p.map(read_wrf, short_dur_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        dur_das[:4] = short_dur_das\n",
    "        man_arrs = np.array([\n",
    "            np.array([\n",
    "                da_lst[i].sel(time=DATA_GROUPS[group][\"ams\"]).max().values \n",
    "                for da_lst in dur_das\n",
    "            ])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "        \n",
    "        # read AMS from pipeline step\n",
    "        ams_fps = [\n",
    "            sorted(glob.glob(os.path.join(ams_dir, f\"*{group}*_{duration}*.nc\")))[0]\n",
    "            for duration in DURATIONS\n",
    "        ]     \n",
    "        # spread reading over multiple cores\n",
    "        ams_args = [(fp, wrf_xy) for fp in ams_fps]\n",
    "        p = Pool(15)\n",
    "        ams_das = p.map(read_wrf, ams_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        # make arrays that are comparable to manually computed results\n",
    "        ams_arrs = np.array([\n",
    "            np.array([da_lst[i].values[0] for da_lst in ams_das])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "        \n",
    "        result = np.all(np.isclose(ams_arrs, man_arrs))\n",
    "        results[group] = {\"result\": result, \"ams_das\": ams_das}\n",
    "        if result:\n",
    "            print(f\"Result: PASS\")\n",
    "        else:\n",
    "            print(f\"Result: FAIL\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_estimates(data):\n",
    "    \"\"\"\n",
    "    take data as numpy array, return estimates and CIs\n",
    "    \"\"\"\n",
    "    def run_bootstrap(data, lmom_fitted, intervals, method='pi'):\n",
    "        ''' \n",
    "        Calculate confidence intervals using parametric bootstrap and the\n",
    "        percentile interval method\n",
    "        '''\n",
    "        # function to bootstrap\n",
    "        def sample_return_intervals(data, intervals=intervals):\n",
    "            sample = lmom_fitted.rvs(len(data))\n",
    "            paras = distr.gev.lmom_fit(sample)\n",
    "            samplefit = [ paras[i] for i in ['loc', 'scale', 'c']]\n",
    "            sample_fitted = distr.gev(**paras)\n",
    "            sample_intervals = sample_fitted.ppf(1.0-1./intervals)\n",
    "            res = samplefit\n",
    "            res.extend(sample_intervals.tolist())\n",
    "            return tuple(res)\n",
    "\n",
    "        # the calculation\n",
    "        out = boot.ci(\n",
    "            data, statfunction=sample_return_intervals,\n",
    "            alpha=0.05, n_samples=5000, \n",
    "            method=method, output='lowhigh'\n",
    "        )\n",
    "        ci_Td = out[0, 3:]\n",
    "        ci_Tu = out[1, 3:]\n",
    "        params_ci ={}\n",
    "\n",
    "        return {'ci_Td':ci_Td, 'ci_Tu':ci_Tu}\n",
    "    \n",
    "    data = data * 0.0393701\n",
    "    paras = distr.gev.lmom_fit(data)\n",
    "    fitted_gev = distr.gev(**paras)\n",
    "    avi = np.array([2,5,10,25,50,100,200,500,1000]).astype(np.float)    \n",
    "    estimates = fitted_gev.ppf(1.-1./avi)\n",
    "    boot_out = run_bootstrap(data, fitted_gev, avi)\n",
    "    \n",
    "    # return array with shape (3, 9): lower diff, estimate, upper diff\n",
    "    lower = boot_out[\"ci_Td\"] - estimates\n",
    "    upper = boot_out[\"ci_Tu\"] - estimates\n",
    "    return np.array([lower, estimates, upper])\n",
    "\n",
    "\n",
    "def run_intervals_test(ams_results, itl_dir, wrf_xy):\n",
    "    \"\"\"\n",
    "    Verify that intervals are reproducible for sample locations\n",
    "    \"\"\"    \n",
    "    results = {}\n",
    "    for group in DATA_GROUPS:\n",
    "        # compute intervals manually from AMS test output\n",
    "        print(f\"  Working on {group}...\", end=\"\")\n",
    "        ams_das = ams_results[group][\"ams_das\"]\n",
    "        # lists of arrays of data\n",
    "        data_arrs = [[da_lst[i].values for da_lst in ams_das] for i in np.arange(2)]\n",
    "        # iterate through test locations and Pool the estimation\n",
    "        man_arrs = []\n",
    "        for arr_lst in data_arrs:\n",
    "            p = Pool(15)\n",
    "            man_arrs.append(np.array(p.map(run_estimates, arr_lst)))\n",
    "            p.close()\n",
    "            p.join()\n",
    "        man_arrs = np.array(man_arrs)\n",
    "        \n",
    "        # read the \"diff'd\" estimates from pipeline step\n",
    "        itl_fps = [\n",
    "            sorted(glob.glob(os.path.join(itl_dir, f\"*{group}*_{duration}*.nc\")))[0]\n",
    "            for duration in DURATIONS\n",
    "        ]\n",
    "        # spread reading over multiple cores\n",
    "        itl_args = [(fp, wrf_xy) for fp in itl_fps]\n",
    "        p = Pool(15)\n",
    "        itl_das = p.map(read_wrf, itl_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        # make arrays that are comparable to manually computed results\n",
    "        varnames = [\"pf-lower\", \"pf\", \"pf-upper\"]\n",
    "        itl_arrs = np.array([\n",
    "            np.array([\n",
    "                np.array([da_lst[i][varname].values for varname in varnames])\n",
    "                for da_lst in itl_das\n",
    "            ])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "        \n",
    "        # since the resulting confidence bounds are not \n",
    "        # determined (randomness of bootstrap), compare\n",
    "        # the pf estimate results much more closely\n",
    "        # pf estimate arrays\n",
    "        man_pf_arr = np.array([\n",
    "            [est_arr[1] for est_arr in dur_arr] \n",
    "            for dur_arr in man_arrs\n",
    "        ])\n",
    "        itl_pf_arr = np.array([\n",
    "            [est_arr[1] for est_arr in dur_arr] \n",
    "            for dur_arr in itl_arrs\n",
    "        ])\n",
    "        # conf bound diffs arrays\n",
    "        man_ci_arr = np.array([\n",
    "            [np.array([est_arr[0], est_arr[2]]) for est_arr in dur_arr] \n",
    "            for dur_arr in man_arrs\n",
    "        ])\n",
    "        itl_ci_arr = np.array([\n",
    "            [np.array([est_arr[0], est_arr[2]]) for est_arr in dur_arr] \n",
    "            for dur_arr in itl_arrs\n",
    "        ])\n",
    "\n",
    "        pf_result = np.all(np.isclose(itl_pf_arr, man_pf_arr))\n",
    "        ci_result = np.all(\n",
    "            np.isclose(\n",
    "                np.around(itl_pf_arr, 2), np.round(man_pf_arr, 2)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        result = np.all([pf_result, ci_result])\n",
    "        results[group] = {\"result\": result, \"itl_das\": itl_das}\n",
    "        if result:\n",
    "            print(f\"Result: PASS\")\n",
    "        else:\n",
    "            print(f\"Result: FAIL\")\n",
    "            \n",
    "    return results\n",
    "        \n",
    "    \n",
    "def run_deltas_warp_test(itl_results, deltas_dir, warp_dir, wrf_xy, a14_xy):\n",
    "    \"\"\"\n",
    "    Verify that deltas and warping are reproducible\n",
    "    \"\"\" \n",
    "    results = {}\n",
    "    # need to iterate over only models now\n",
    "    for group in [\"GFDL-CM3\", \"NCAR-CCSM4\"]:\n",
    "        print(f\"Working on {group}...\", end=\"\")\n",
    "        itl_das = itl_results[group + \"_historical\"][\"itl_das\"]\n",
    "        # deltas only calculated from pf\n",
    "        itl_arrs = np.array([\n",
    "            np.array([\n",
    "                da_lst[i][\"pf\"].values\n",
    "                for da_lst in itl_das\n",
    "            ])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "        # read in future period intervals data\n",
    "        fut_itl_fps = [\n",
    "            sorted(glob.glob(os.path.join(itl_dir, f\"*{group}_rcp85*_{duration}*.nc\")))[0]\n",
    "            for duration in DURATIONS\n",
    "        ]\n",
    "        # spread reading over multiple cores\n",
    "        fut_itl_args = [(fp, wrf_xy) for fp in fut_itl_fps]\n",
    "        p = Pool(15)\n",
    "        fut_itl_das = p.map(read_wrf, fut_itl_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        # stuff into arrays\n",
    "        fut_itl_arrs = np.array([\n",
    "            np.array([\n",
    "                da_lst[i][\"pf\"].values\n",
    "                for da_lst in fut_itl_das\n",
    "            ])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "        # take deltas\n",
    "        man_arrs = fut_itl_arrs / itl_arrs\n",
    "\n",
    "        # read the deltas from pipeline step\n",
    "        deltas_fps = [\n",
    "            sorted(glob.glob(os.path.join(deltas_dir, f\"*{group}*_{duration}*.nc\")))[0]\n",
    "            for duration in DURATIONS\n",
    "        ]\n",
    "        # spread reading over multiple cores\n",
    "        deltas_args = [(fp, wrf_xy) for fp in deltas_fps]\n",
    "        p = Pool(15)\n",
    "        deltas_das = p.map(read_wrf, deltas_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        # make arrays that are comparable to manually computed results\n",
    "        deltas_arrs = np.array([\n",
    "            np.array([\n",
    "                da_lst[i][\"pf\"].values\n",
    "                for da_lst in deltas_das\n",
    "            ])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "\n",
    "        deltas_result = np.all(np.isclose(deltas_arrs, man_arrs))\n",
    "\n",
    "        # read the warped estimates from pipeline step\n",
    "        warp_fps = [\n",
    "            sorted(glob.glob(os.path.join(warp_dir, f\"*{group}*_{duration}*.nc\")))[0]\n",
    "            for duration in DURATIONS\n",
    "        ]\n",
    "        # spread reading over multiple cores\n",
    "        warp_args = [(fp, a14_xy) for fp in warp_fps]\n",
    "        p = Pool(15)\n",
    "        warp_das = p.map(read_wrf, warp_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        # make arrays that are comparable to manually computed results\n",
    "        warp_arrs = np.array([\n",
    "            np.array([\n",
    "                da_lst[i][\"pf\"].values\n",
    "                for da_lst in warp_das\n",
    "            ])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "        # Now, ensure that the euclidean distance between corresponding arrays\n",
    "        # are the smallest in comparison to all other combinations\n",
    "        # probably not the best way to check this, but it should be the case that\n",
    "        # distances between warped deltas and deltas, queried at the same relative location,\n",
    "        # should be smaller than randomly chosen locations (represented by the locations\n",
    "        # of other points)\n",
    "        check_dists = [np.linalg.norm(x-y) for x,y in zip(warp_arrs, deltas_arrs)]\n",
    "        rand_dists = [\n",
    "            np.linalg.norm(x-y) for x,y in zip(warp_arrs, [\n",
    "                np.roll(deltas_arrs, i, 0) \n",
    "                for i in np.arange(deltas_arrs.shape[0])[1:]\n",
    "            ])\n",
    "        ]\n",
    "        warp_result = np.all([[d1 < d2 for d1 in check_dists] for d2 in rand_dists])\n",
    "        \n",
    "        result = np.all([deltas_result, warp_result])\n",
    "        results[group] = {\"result\": result, \"warp_das\": warp_das}\n",
    "        if result:\n",
    "            print(f\"Result: PASS\")\n",
    "        else:\n",
    "            print(f\"Result: FAIL\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_multiply_test(warp_results, mul_dir, a14_xy):\n",
    "    \"\"\"\n",
    "    Verify that results of multiplication step are reproducible\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for group in [\"GFDL-CM3\", \"NCAR-CCSM4\"]:\n",
    "        print(f\"working on {group}...\", end=\"\")\n",
    "        # make warp arrs for multiplying with a14 data\n",
    "        warp_das = warp_results[group][\"warp_das\"]\n",
    "        warp_arrs = np.array([\n",
    "            np.array([\n",
    "                da_lst[i][\"pf\"].values\n",
    "                for da_lst in warp_das\n",
    "            ])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "        # read atlas 14 data\n",
    "        a14_durations = [duration.replace(\"2d\", \"48h\") for duration in DURATIONS]\n",
    "        a14_fps = [\n",
    "            os.path.join(a14_dir, f\"ak{interval}yr{duration.zfill(3)}a_ams.tif\")\n",
    "            for duration in a14_durations\n",
    "            for interval in intervals\n",
    "        ]\n",
    "        a14_args = [(fp, windows) for fp in a14_fps]\n",
    "        with rio.open(a14_fps[0]) as src:\n",
    "            rc_idx = [src.index(*xy) for xy in a14_xy]\n",
    "        windows = [Window(idx[1], idx[0], 1, 1) for idx in rc_idx]\n",
    "\n",
    "        # pool reading of atlas14 data\n",
    "        p = Pool(20)\n",
    "        a14_data = p.map(read_a14, a14_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        a14_arrs = np.array([\n",
    "            np.array([data[i] for data in a14_data]).reshape(14, 9)\n",
    "            for i in np.arange(len(wgs84_coords))\n",
    "        ])\n",
    "        # manually computed multiplied\n",
    "        man_arrs = a14_arrs * warp_arrs\n",
    "        \n",
    "        # read multiplied data from pipelines\n",
    "        mul_fps = [\n",
    "            sorted(glob.glob(os.path.join(mul_dir, f\"*{group}*_{duration}*\")))[0]\n",
    "            for duration in DURATIONS\n",
    "        ]\n",
    "        mul_args = [(fp, a14_xy) for fp in mul_fps]\n",
    "        p = Pool(15)\n",
    "        mul_das = p.map(read_wrf, mul_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        # stuff into arrays\n",
    "        mul_arrs = np.array([\n",
    "            np.array([\n",
    "                da_lst[i][\"pf\"].values / 1000\n",
    "                for da_lst in mul_das\n",
    "            ])\n",
    "            for i in np.arange(2)\n",
    "        ])\n",
    "    \n",
    "        result = np.all(np.isclose(mul_arrs, man_arrs))\n",
    "        if result:\n",
    "            print(f\"Result: PASS\")\n",
    "        else:\n",
    "            print(f\"Result: FAIL\")\n",
    "            \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/UA/kmredilla/.localpython/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os, glob, time, datetime\n",
    "import lmoments3 as lmom\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import scikits.bootstrap as boot\n",
    "import xarray as xr\n",
    "from lmoments3 import distr\n",
    "from multiprocessing import Pool\n",
    "from pyproj import Transformer\n",
    "from rasterio.windows import Window\n",
    "\n",
    "\n",
    "# define global variables\n",
    "# because of the asymmetry in file structure between short/long durations, \n",
    "#   simplest way to efficiently compare manual sample with pipeline is to load\n",
    "#   first years used in each step for each group \n",
    "#   (1970, 2006 for durations; 1979, 2020 for AMS)\n",
    "DATA_GROUPS = {\n",
    "    \"GFDL-CM3_historical\": {\"dur\": \"1970\", \"ams\": \"1979\"},\n",
    "    \"GFDL-CM3_rcp85\": {\"dur\": \"2006\", \"ams\": \"2020\"},\n",
    "    \"NCAR-CCSM4_historical\": {\"dur\": \"1970\", \"ams\": \"1979\"},\n",
    "    \"NCAR-CCSM4_rcp85\": {\"dur\": \"2006\", \"ams\": \"2020\"},\n",
    "}\n",
    "\n",
    "DURATIONS = [\n",
    "    \"60m\",\n",
    "    \"2h\",\n",
    "    \"3h\",\n",
    "    \"6h\",\n",
    "    \"12h\",\n",
    "    \"24h\",\n",
    "    \"2d\",\n",
    "    \"3d\",\n",
    "    \"4d\",\n",
    "    \"7d\",\n",
    "    \"10d\",\n",
    "    \"20d\",\n",
    "    \"30d\",\n",
    "    \"45d\",\n",
    "    \"60d\",\n",
    "]\n",
    "DURATIONS_PANDAS = [\n",
    "    \"1H\",\n",
    "    \"2H\",\n",
    "    \"3H\",\n",
    "    \"6H\",\n",
    "    \"12H\",\n",
    "    \"24H\",\n",
    "    \"2D\",\n",
    "    \"3D\",\n",
    "    \"4D\",\n",
    "    \"7D\",\n",
    "    \"10D\",\n",
    "    \"20D\",\n",
    "    \"30D\",\n",
    "    \"45D\",\n",
    "    \"60D\",\n",
    "]\n",
    "\n",
    "pipe_dir = os.getenv(\"PIPE_DIR\")\n",
    "wrf_xy = transform_wgs84(wgs84_coords)\n",
    "a14_xy = transform_wgs84(wgs84_coords, wrf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on GFDL-CM3_historical...Result: PASS\n",
      "Working on GFDL-CM3_rcp85...Result: PASS\n",
      "Working on NCAR-CCSM4_historical...Result: PASS\n",
      "Working on NCAR-CCSM4_rcp85...Result: PASS\n",
      "Elapsed time: 16.1 m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrf_dir = os.path.join(pipe_dir, \"pcpt\")\n",
    "dur_dir = os.path.join(pipe_dir, \"durations\")\n",
    "\n",
    "\n",
    "args = (wrf_dir, dur_dir, wrf_xy)\n",
    "dur_results = timeit(run_durations_test, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annual maximums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on GFDL-CM3_historical...Result: PASS\n",
      "Working on GFDL-CM3_rcp85...Result: PASS\n",
      "Working on NCAR-CCSM4_historical...Result: PASS\n",
      "Working on NCAR-CCSM4_rcp85...Result: PASS\n",
      "Elapsed time: 22.4 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ams_dir = os.path.join(pipe_dir, \"ams\")\n",
    "\n",
    "args = (dur_results, dur_dir, ams_dir, wrf_xy)\n",
    "ams_results = timeit(run_ams_test, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Working on GFDL-CM3_historical...Result: PASS\n",
      "  Working on GFDL-CM3_rcp85...Result: PASS\n",
      "  Working on NCAR-CCSM4_historical...Result: PASS\n",
      "  Working on NCAR-CCSM4_rcp85...Result: PASS\n",
      "Elapsed time: 2.0 m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itl_dir = os.path.join(pipe_dir, \"diff\")\n",
    "\n",
    "args = (ams_results, itl_dir, wrf_xy)\n",
    "itl_results = timeit(run_intervals_test, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deltas and warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on GFDL-CM3...Result: PASS\n",
      "Working on NCAR-CCSM4...Result: PASS\n",
      "Elapsed time: 11.1 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltas_dir = os.path.join(pipe_dir, \"deltas\")\n",
    "warp_dir = os.path.join(pipe_dir, \"warp\")\n",
    "\n",
    "args = (itl_results, deltas_dir, warp_dir, wrf_xy, a14_xy)\n",
    "warp_results = timeit(run_deltas_warp_test, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on GFDL-CM3...Result: PASS\n",
      "Working on NCAR-CCSM4...Result: PASS\n",
      "Elapsed time: 1.9 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mul_dir = os.path.join(pipe_dir, \"multiply\")\n",
    "\n",
    "args = (itl_results, deltas_dir, warp_dir, wrf_xy, a14_xy)\n",
    "warp_results = timeit(run_deltas_warp_test, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion time of previous test: 2020-11-30 18:04:35\n"
     ]
    }
   ],
   "source": [
    "utc_time = datetime.datetime.utcnow()\n",
    "comp_time = utc_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"Completion time of previous test: {comp_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
